\section{极大似然估计法}
%@see: 《数理统计教程》（王兆军，邹长亮） P49 定义2.3.1（似然函数）
%@see: 《数理统计教程》（王兆军，邹长亮） P49 定义2.3.2（MLE）

\subsection{对离散型总体的极大似然估计}
设总体\(X\)是离散型随机变量，分布律为\(P(X=x)=p(x,\theta)\)\footnote{
	我们其实应该把\(p(x,\theta)\)看成
	“未知参数\(\theta\)取定为某个值的条件下
	\(X\)的条件概率分布”.
}，
其中\(\theta\)是未知参数.
当样本\(\AutoTuple{X}{n}\)得到一组观测值\(\AutoTuple{x}{n}\)时，
由样本的独立同分布性，
样本取得这组观测值的概率为\begin{equation*}
	P\left[ \bigcap_{i=1}^n (X_i=x_i) \right]
	= \prod_{i=1}^n P(X_i=x_i)
	= \prod_{i=1}^n p(x_i,\theta).
\end{equation*}
可以看出，一旦选定了概率模型（例如二项分布、几何分布等），
那么在观测值\(\AutoTuple{x}{n}\)固定不变的情况下，
未知参数\(\theta\)的取值完全决定了这\(n\)个样本取得观测值的联合概率，
或者说样本取得这组观测值的概率
\(P\left[ \bigcap_{i=1}^n (X_i=x_i) \right]\)就成为了关于\(\theta\)的函数.
我们把函数\begin{equation*}
	L\colon \Theta \to \mathbb{R},
	\theta \mapsto P\left[ \bigcap_{i=1}^n (X_i=x_i) \right]
\end{equation*}
称为“未知参数\(\theta\)的\DefineConcept{似然函数}（likelihood function）”，
%@see: https://mathworld.wolfram.com/LikelihoodFunction.html
把函数\begin{equation*}
	\ln L\colon \Theta \to \mathbb{R}, \theta \mapsto \ln L(\theta)
\end{equation*}
称为“未知参数\(\theta\)的\DefineConcept{对数似然函数}（log-likelihood function）”.

\DefineConcept{极大似然估计法}（maximum likelihood method）的思想是：
%@see: https://mathworld.wolfram.com/MaximumLikelihood.html
随机试验有若干个可能结果，如果在一次试验中某一结果出现了，
由小概率事件原理，我们便自然认为这一结果出现的概率较大，
从而可以认为这一结果是所有可能结果中出现概率最大的一个.

按照极大似然估计法所秉持的思想，
未知参数\(\theta\)应该这样估计：
选择\(\hat{\theta}\)作为\(\theta\)的估计值，使得观测值\(\AutoTuple{x}{n}\)出现的概率最大，
也就是使\(\max_\theta L(\theta) = L(\hat{\theta})\)
或\(\hat{\theta} = \argmax_\theta L(\theta)\).
而求\(L\)的最大值点\(\hat{\theta}\)，
可由方程\(\dv{\theta} L(\theta) = 0\)解得.

\begin{definition}
设总体\(X\)仅含一个未知参数\(\theta\)，
并且总体的分布律或密度函数已知.
假设我们已经取得一组样本观测值\(\AutoTuple{x}{n}\).
若存在\(\hat{\theta}\)使得\begin{equation*}
	L(\hat{\theta}) = \max_\theta L(\theta),
\end{equation*}
则把\(\hat{\theta}(\AutoTuple{x}{n})\)称为
“未知参数\(\theta\)的\DefineConcept{极大似然估计值}”，
而把统计量\(\hat{\theta}(\AutoTuple{X}{n})\)称为
“未知参数\(\theta\)的\DefineConcept{极大似然估计量}（maximum likelihood estimator）”.
%@see: https://online.stat.psu.edu/stat415/lesson/1/1.2
%@see: https://mathworld.wolfram.com/MaximumLikelihoodEstimator.html
\end{definition}

\subsection{对连续型总体的极大似然估计}
对于连续型总体\(X\)，\(X\)的概率密度函数为\(f(x,\theta)\)，
其中\(\theta\)是未知参数.若取得样本观测值\(\AutoTuple{x}{n}\)，
则因为随机变量\(X_i\)落在点\(x_i\)的邻域
（设其长度为\(\increment x_i\)）
内的概率近似于\begin{equation*}
	f(x_i,\theta) \increment x_i
	\quad(i=1,2,\dotsc,n),
\end{equation*}
则样本\((\AutoTuple{X}{n})\)落在样本观测值\((\AutoTuple{x}{n})\)邻域的概率近似为
\(\prod_{i=1}^n f(x_i,\theta) \increment x_i\).
那么\(\theta\)的估计值\(\hat{\theta}\)
应使概率\(\prod_{i=1}^n f(x_i,\theta) \increment x_i\)达到最大值.
但因为\(\increment x_i\)与\(\theta\)无关，
故只要使\(\prod_{i=1}^n{f(x_i,\theta)}\)达到最大值即可.
此时，记\begin{equation*}
	L(\theta) \defeq \prod_{i=1}^n{f(x_i,\theta)},
\end{equation*}
仍然称\(L(\theta)\)为似然函数.

\begin{definition}
用于解出使\(L(\theta)\)取得最大值的极大似然估计值\(\hat{\theta}\)的方程\begin{equation*}
	\dv{\theta} L(\theta) = 0
\end{equation*}称为\DefineConcept{似然方程}.

由于\(\ln L\)和\(L\)在相同的\(\theta\)处取得最大值，有时候也采用方程\begin{equation*}
	\dv{\theta} \ln L(\theta) = 0,
\end{equation*}
以解出极大似然估计值\(\hat{\theta}\)，
并称之为\DefineConcept{对数似然方程}.
\end{definition}

当总体\(X\)服从单峰分布\footnote{%
“单峰分布”是指密度函数图像或其概率分布图只有一个峰的分布.
除均匀分布以外，常见分布都是单峰分布.}时，
若似然方程或对数似然方程有解，
则其解就是\(\theta\)的极大似然估计值.

\begin{example}
%@see: 《概率论与数理统计》（陈鸿建、赵永红、翁洋） P196 例8.6
设总体\(X \sim B(N,p)\)，
其中\(N\)已知，
\(\AutoTuple{x}{n}\)为样本观测值，
求\(p\)及\(m=E(X)\)的极大似然估计.
\begin{solution}
总体\(X\)的分布律为\begin{equation*}
	f(x,p)
	= C_N^x p^x (1-p)^{N-x},
	\quad x=0,1,\dots,N.
\end{equation*}
故似然函数为\begin{equation*}
	L(p)
	= \prod_{i=1}^n f(x_i,p)
	= \prod_{i=1}^n C_N^{x_i} p^{x_i} (1-p)^{N-x_i}
	= p^{\sum_{i=1}^n x_i}
		(1-p)^{nN-\sum_{i=1}^n x_i}
		\prod_{i=1}^n C_N^{x_i},
\end{equation*}
取对数，得\begin{equation*}
	\ln L(p)
	= \sum_{i=1}^n x_i \ln p
	+ \left(nN - \sum_{i=1}^n{x_i}\right) \ln(1-p)
	+ \sum_{i=1}^n \ln C_N^{x_i}.
\end{equation*}
求导得\begin{equation*}
	\dv{p} \ln L(p)
	= \frac{1}{p} \sum_{i=1}^n{x_i}
	- \frac{1}{1-p} \left(nN - \sum_{i=1}^n{x_i}\right).
\end{equation*}
建立对数似然方程\(\dv{p} \ln L(p) = 0\)，
解得\(p\)的极大似然估计值为\begin{equation*}
	\hat{p}
	= \frac{1}{nN} \sum_{i=1}^n x_i
	= \frac{\overline{x}}{N},
\end{equation*}
其中\(\overline{x}=\frac{1}{n}\sum_{i=1}^n{x_i}\).
而\(p\)的极大似然估计量为\begin{equation*}
	\hat{p} = \frac{\overline{X}}{N}.
\end{equation*}

又因为\(m=E(X)=Np\)，
故\(m\)的极大似然估计值为\begin{equation*}
	\hat{m} = N\hat{p} = \overline{x},
\end{equation*}
而\(m\)的极大似然估计量为\begin{equation*}
	\hat{m} = \overline{X}.
\end{equation*}
\end{solution}
\end{example}

注意当\(N\)已知时，二项分布\(B(N,p)\)属于自然指数分布族.
这个例子关于“\(m\)的极大似然估计量为\(\overline{X}\)”的结论
对一般的自然指数分布族也成立.

\begin{theorem}
%@see: 《概率论与数理统计》（陈鸿建、赵永红、翁洋） P197 定理8.1
若总体\(X\)服从自然指数分布族分布，
则均值参数\(m=E(X)\)的极大似然估计量为样本均值，
即\(\hat{m}=\overline{X}\).
\begin{proof}
总体\(X\)的概率分布或密度函数为
\(f(x,\theta)=e^{\theta x - \phi(\theta)} h(x)\)，
取得样本观测值\(\AutoTuple{x}{n}\)，
注意\(\theta\)是\(m\)的函数\(\theta(m)\)，
故得似然函数\begin{equation*}
	L(m) = \prod_{i=1}^n e^{\theta(m) x_i -\phi[\theta(m)]} h(x_i)
	= \exp\left\{
		\theta(m) \sum_{i=1}^n[x_i - n \phi(\theta(m))]
		\prod_{i=1}^n{h(x_i)}
	\right\},
\end{equation*}
再取对数得\begin{equation*}
	\ln L(m)
	= \theta(m) \sum_{i=1}^n x_i - n \phi[\theta(m)]
	+ \ln \prod_{i=1}^n{h(x_i)}.
\end{equation*}

令\begin{equation*}
	\dv{m} \ln L(m)
	= \theta'(\theta) \sum_{i=1}^n{x_i}
	- n \phi'[\theta(m)] \theta'(m) = 0,
\end{equation*}
由于\(\phi'(\theta) = m\)，
\(\dv{m}{\theta} = \phi''(\theta) = D(X) > 0\)，
从而\(\theta'(m) = \dv{\theta}{m} > 0\)，
所以\begin{equation*}
	\sum_{i=1}^n{x_i} - nm = 0.
\end{equation*}
可见\(m\)的极大似然估计值为
\(\hat{m} = \overline{x}\)，
其极大似然估计量为
\(\hat{m} = \overline{X}\).
\end{proof}
\end{theorem}

这样，对自然指数分布族，
均值参数\(m\)的矩估计量与极大似然估计量都是样本均值\(\overline{X}\).
而且，当总体方差函数\(\sigma^2=V(m)\)有单值反函数时，
方差函数\(V(m)\)的极大似然估计量为\(V(\overline{X})\).
不难证明，在常见的自然指数分布族分布中，除去二项分布外，
它们的方差函数\(V(m)\)在其均值空间中都有单值反函数.
比如，对几何分布，\(m=E(X)=\frac{1}{p}\)，\(V(m)=D(X)=\frac{q}{p^2}=m^2-m\)，
在\(m>1\)时有单值反函数，
故\(V(m)=m^2-m\)的极大似然估计为\(V(\overline{X})=\overline{X}^2 - \overline{X}\).

当总体\(X\)的分布中含有多个未知参数，
即\(\vb{\theta}=(\AutoTuple{\theta}{k})\)时，
似然函数为\begin{equation*}
	L(\vb{\theta})
	= L(\AutoTuple{\theta}{k}).
\end{equation*}
于是我们有若干个对数似然方程，
需要建立对数似然方程组\begin{equation*}
	\def\g#1{\pdv{\theta_{#1}} \ln L(\vb{\theta}) = 0}
	\left\{ \def\arraystretch{1.5} \begin{array}{l}
		\g{1}, \\
		\g{2}, \\
		\hdotsfor{1} \\
		\g{k}. \\
	\end{array} \right.
\end{equation*}
若对数似然方程组有解\(\hat{\vb\theta}=(\AutoTuple{\hat{\theta}}{k})\)，
则它们分别是\(\vb\theta=(\AutoTuple{\theta}{k})\)的极大似然估计值.

\begin{example}
%@see: 《概率论与数理统计》（陈鸿建、赵永红、翁洋） P198 例8.7
设总体\(X \sim N(\mu,\sigma^2)\)，
其中\(\mu\)和\(\sigma^2\)都是未知参数.
\(\AutoTuple{x}{n}\)为样本观测值.
求\(\mu\)和\(\sigma^2\)的极大似然估计.
\begin{solution}
似然函数为\begin{align*}
	L(\mu,\sigma^2)
	&= \prod_{i=1}^n f(x_i,\mu,\sigma^2)
	= \prod_{i=1}^n
		\frac{1}{\sqrt{2\pi}\sigma}
		e^{-\frac{(x_i-\mu)^2}{2\sigma^2}} \\
	&= (2\pi\sigma^2)^{-\frac{n}{2}}
		\exp[-\frac{1}{2\sigma^2} \sum_{i=1}^n{(x_i-\mu)^2}],
\end{align*}
取对数，得\begin{equation*}
	\ln L(\mu,\sigma^2)
	= -\frac{n}{2} (\ln{2\pi} + \ln \sigma^2)
	- \frac{1}{2\sigma^2} \sum_{i=1}^n{(x_i-\mu)^2},
\end{equation*}
从而有\begin{equation*}
	\left\{ \begin{array}{l}
		\pdv{\mu} \ln L(\mu,\sigma^2) = \frac{1}{\sigma^2} \sum_{i=1}^n{(x_i-\mu)} = 0, \\
		\pdv{(\sigma^2)} \ln L(\mu,\sigma^2) = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^n{(x_i-\mu)^2} = 0.
	\end{array} \right.
\end{equation*}
解得\(\mu\)及\(\sigma^2\)的极大似然估计值为\begin{equation*}
	\left\{ \begin{array}{l}
	\hat{\mu} = \overline{x}, \\
	\hat{\sigma^2} = \frac{1}{n} \sum_{i=1}^n{(x_i-\overline{x})^2} = b_2,
	\end{array} \right.
\end{equation*}
而其极大似然估计量为\begin{equation*}
	\left\{ \begin{array}{l}
		\hat{\mu} = \overline{X}, \\
		\hat{\sigma^2} = \frac{1}{n} \sum_{i=1}^n{(X_i-\overline{X})^2} = B_2.
	\end{array} \right.
\end{equation*}
\end{solution}
\end{example}
\begin{remark}
由上可知，当总体\(X \sim N(\mu,\sigma^2)\)时，
\(\mu\)和\(\sigma^2\)的矩估计量与极大似然估计量是相同的.
\end{remark}
\begin{remark}
需要指出的是，当似然方程或对数似然方程无解时，应从定义考虑求极大似然估计，
即选择\(\hat{\theta}\)使得\(L(\hat{\theta})=\max L(\theta)\).
\end{remark}

\begin{example}
设总体\(X \sim U(0,\theta)\)，\(\theta>0\)是未知参数，
\(\AutoTuple{x}{n}\)是样本观测值.
求\(\theta\)的极大似然估计.
\begin{solution}
\(X\)的密度函数为\begin{equation*}
	f(x,\theta) = \left\{ \begin{array}{cl}
		\theta^{-1}, & 0 \leq x \leq \theta, \\
		0, & \text{其他},
	\end{array} \right.
\end{equation*}
而似然函数为\begin{equation*}
	L(\theta) = \prod_{i=1}^n{f(x_i,\theta)} = \theta^{-n},
	\quad x_i \in [0,\theta], \quad i=1,2,\dotsc,n.
\end{equation*}
由于似然方程\(\dv{\theta} L(\theta) = -n\theta^{-1-n} = 0\)在\(\theta>0\)无解.
所以应该考虑似然估计的定义.
因为似然函数\(L(\theta)=\theta^{-n}\)在\(\theta>0\)时为\(\theta\)的单调递减函数，
\(\theta\)越小则\(L(\theta)\)越大；
但另一方面，\(x_i\in[0,\theta]\)，
故有\(\max_{1 \leq i \leq n} x_i \in [0,\theta]\).
故当\(\hat{\theta}=\max_{1 \leq i \leq n} x_i\)时，
\(L(\hat{\theta})=\max L(\theta)\).
所以，\(\theta\)的极大似然估计值为\begin{equation*}
	\hat{\theta} = \max_{1 \leq i \leq n} x_i,
\end{equation*}
而其极大似然估计量为\begin{equation*}
	\max_{1 \leq i \leq n} X_i.
\end{equation*}
\end{solution}
\end{example}

\begin{example}
%@see: 《2022年全国硕士研究生入学统一考试（数学一）》三解答题/第22题
设\(\AutoTuple{X}{n}\)是来自均值为\(\theta\)的指数分布总体的简单随机样本，
\(\AutoTuple{Y}{m}\)是来自均值为\(2\theta\)的指数分布总体的简单随机样本，
且两样本相互独立，其中\(\theta\ (\theta>0)\)是未知参数.
利用\(\AutoTuple{X}{n},\AutoTuple{Y}{m}\)求\(\theta\)的最大似然估计量\(\theta\)，
并求\(D(\hat\theta)\).
\begin{solution}
由题意有，均值为\(\theta\)的指数分布函数为\begin{equation*}
	f_X(x) = \frac1\theta e^{-x/\theta}
	\quad(x>0),
\end{equation*}
均值为\(2\theta\)的指数分布函数为\begin{equation*}
	f_Y(y) = \frac1{2\theta} e^{-y/2\theta}
	\quad(y>0),
\end{equation*}
似然函数为\begin{equation*}
	L(\theta) = \frac1{2^m \theta^{n+m}} \exp\left(
		-\frac1\theta \sum_{i=1}^n x_i
		-\frac1{2\theta} \sum_{j=1}^m y_j
	\right)
	\quad(x_i>0,y_j>0,i=1,2,\dotsc,n,j=1,2,\dotsc,m).
\end{equation*}
取对数，得\begin{equation*}
	\ln L(\theta) = -m\ln2 -(m+n)\ln\theta
	- \frac1\theta \sum_{i=1}^n x_i
	-\frac1{2\theta} \sum_{j=1}^m y_j.
\end{equation*}
令\(\dv\theta \ln L(\theta) = 0\)，得\begin{equation*}
	\theta = \frac1{2(m+n)} \left( 2 \sum_{i=1}^n x_i + \sum_{j=1}^m y_j \right).
\end{equation*}
于是\(\hat\theta = \frac1{2(m+n)} \left( 2 \sum_{i=1}^n X_i + \sum_{j=1}^m Y_j \right)\)
就是\(\theta\)的最大似然估计量.
那么\begin{equation*}
	D(\hat\theta)
	= \frac1{(m+n)^2} \sum_{i=1}^n D(X_i) + \frac1{4(m+n)^2} \sum_{j=1}^m D(Y_j)
	= \frac{\theta^2}{m+n}.
\end{equation*}
\end{solution}
\end{example}
